{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppo_pong.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yGGVWC2gCfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# http://karpathy.github.io/2016/05/31/rl/\n",
        "def preprocess_image(img):\n",
        "  # shape = 210x160x3\n",
        "  img = img[35:195] # crop -> 160 x 160 x 3\n",
        "  img = img[::2,::2,0] # downsample by factor of 2 -> 80 x 80\n",
        "  img[img == 144] = 0 # erase background (background type 1)\n",
        "  img[img == 109] = 0 # erase background (background type 2)\n",
        "  img[img != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return img\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ktd3tkYjqaNK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "5f202347-7273-4cb2-d354-6ac2fa38b66e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "import gym\n",
        "\n",
        "env = gym.make('PongNoFrameskip-v4')\n",
        "frame = env.reset()\n",
        "frame = env.step(0)\n",
        "# neki random frame prikazi\n",
        "for i in range(randint(10, 30)):\n",
        "  frame, r, d, _ = env.step(randint(0, env.action_space.n - 1))\n",
        "  if d:\n",
        "    env.reset()\n",
        "    frame, r, d, _ = env.step(0) # Fire\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(frame)\n",
        "plt.title('original')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.title('preprocessed')\n",
        "preprocessed = preprocess_image(frame)\n",
        "plt.imshow(preprocessed)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(preprocessed.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(80, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT3w_i_wsFKs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0030ce51-55cb-43a7-8adf-1b0556bdbf19"
      },
      "source": [
        "import torch\n",
        "\n",
        "# https://github.com/colinskow/move37/blob/master/dqn/lib/wrappers.py\n",
        "class FireResetWrapper(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"For environments where the user needs to press FIRE for the game to start.\"\"\"\n",
        "        super(FireResetWrapper, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def step(self, action):\n",
        "        return self.env.step(action)\n",
        "\n",
        "    def reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset()\n",
        "        return obs\n",
        "\n",
        "  \n",
        "class SkipAndPoolWrapper(gym.Wrapper):\n",
        "\n",
        "    def __init__(self, env, skip=4, pooling_function='max'):\n",
        "        super(SkipAndPoolWrapper, self).__init__(env)\n",
        "        if pooling_function == 'max':\n",
        "            self.pool = np.max\n",
        "        elif pooling_function == 'mean':\n",
        "            self.pool = np.mean\n",
        "        else:\n",
        "            raise ValueError('Only accepting \"max\" and \"mean\" pooling')\n",
        "\n",
        "        self.buff = np.zeros((skip, *env.observation_space.shape))\n",
        "    \n",
        "    def step(self, act):\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        processed_before_done = 0\n",
        "        for i in range(self.buff.shape[0]):\n",
        "            obs, rw, done, info = self.env.step(act)\n",
        "            total_reward += rw\n",
        "            self.buff[i] = obs\n",
        "            processed_before_done += 1\n",
        "            if done:\n",
        "                break\n",
        "        pooled_frame = self.pool(self.buff[:processed_before_done], axis=0)\n",
        "        return pooled_frame, total_reward, done, info\n",
        "\n",
        "class TorchWrapper(gym.Wrapper):\n",
        "    \n",
        "    def __init__(self, env=None, obs_dtype=torch.float32, reward_dtype=torch.float32):\n",
        "        super(TorchWrapper, self).__init__(env)\n",
        "        self.obs_dtype = obs_dtype\n",
        "        self.reward_dtype = reward_dtype\n",
        "    \n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        return torch.as_tensor(obs, dtype=self.obs_dtype)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        return torch.as_tensor(obs, dtype=self.obs_dtype), torch.as_tensor(reward, dtype=self.reward_dtype), done, info\n",
        "\n",
        "class BufferedWrapper(gym.ObservationWrapper):\n",
        "\n",
        "    def __init__(self, env, steps, dtype=np.float32):\n",
        "        super(BufferedWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(steps, axis=0).reshape(steps, old_space.low.shape[0], old_space.low.shape[1]),\n",
        "                                                old_space.high.repeat(steps, axis=0).reshape(steps, old_space.high.shape[0], old_space.high.shape[1]),\n",
        "                                                dtype=dtype)\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "    \n",
        "    def observation(self, obs):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = obs\n",
        "        return self.buffer\n",
        "\n",
        "class PreprocessWrapper(gym.ObservationWrapper):\n",
        "\n",
        "  def __init__(self, env=None):\n",
        "    super(PreprocessWrapper, self).__init__(env)\n",
        "    obs_space_old = self.observation_space\n",
        "    old_shape = obs_space_old.shape\n",
        "    self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(int((old_shape[0] - 50) / 2), int((old_shape[1]) / 2)))\n",
        "    \n",
        "  \n",
        "  def observation(self, obs):\n",
        "    return preprocess_image(obs)\n",
        "\n",
        "wrapped_env = PreprocessWrapper(env)\n",
        "print(wrapped_env.observation_space.shape)\n",
        "\n",
        "img = wrapped_env.reset()\n",
        "print(img.shape)\n",
        "wrapped_env.step(0)\n",
        "for i in range(randint(10, 30)):\n",
        "  img, r, d, _ = wrapped_env.step(randint(0, env.action_space.n - 1))\n",
        "  if d:\n",
        "    wrapped_env.reset()\n",
        "    img, r, d, _ = wrapped_env.step(0) # Fire\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(80, 80)\n",
            "(80, 80)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXVBG3VXvbhb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eb1c0d36-3dc8-4b7a-b3b9-51d4d55667ba"
      },
      "source": [
        "def make_pong_env(name, use_skip=True):\n",
        "  env = gym.make(name)\n",
        "  if use_skip:\n",
        "    env = SkipAndPoolWrapper(env)\n",
        "  env = FireResetWrapper(env)\n",
        "  env = PreprocessWrapper(env)\n",
        "  env = BufferedWrapper(env, 4)\n",
        "  env = TorchWrapper(env)\n",
        "  return env\n",
        "\n",
        "env = make_pong_env('PongNoFrameskip-v4')\n",
        "obs = env.reset()\n",
        "print(obs.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 80, 80])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3vO5jmuFaYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pdb\n",
        "\n",
        "def train(env,\n",
        "          actor,\n",
        "          critic,\n",
        "          value_opt,\n",
        "          actor_opt,\n",
        "          max_kl=0.01,\n",
        "          epsilon=0.2,\n",
        "          epochs=100,\n",
        "          batch_size=50,\n",
        "          gamma=0.99,\n",
        "          lam=0.96,\n",
        "          val_loss_coef=0.5,\n",
        "          entropy_coeff=0.001,\n",
        "          train_critic_iters=5,\n",
        "          train_actor_iters=5):\n",
        "\n",
        "    \n",
        "    def rewards_and_advantages(rewards, obs, critic):\n",
        "        #pdb.set_trace()\n",
        "        n = len(rewards)\n",
        "        adv_to_go = torch.zeros(n)\n",
        "        rewards_to_go = torch.zeros(n)\n",
        "        #vs = torch.zeros(n)\n",
        "        #for i in range(n):\n",
        "        #   vs[i] = critic(obs[i])\n",
        "        with torch.no_grad():\n",
        "          vs = critic(torch.stack(obs)).squeeze()\n",
        "\n",
        "        for i in reversed(range(n)):\n",
        "            not_last = i < (n - 1)\n",
        "            rewards_to_go[i] = rewards[i]\n",
        "            adv_to_go[i] = rewards[i] - vs[i]\n",
        "\n",
        "            if not_last:\n",
        "                #pdb.set_trace()\n",
        "                adv_to_go[i] += gamma * vs[i + 1] + gamma * lam * adv_to_go[i + 1]\n",
        "                rewards_to_go[i] += gamma * rewards_to_go[i + 1]\n",
        "            \n",
        "        return adv_to_go, rewards_to_go\n",
        "    \n",
        "\n",
        "    def loss_and_kl(obs, adv, actions, old_log_probs):\n",
        "        log_probs, entropy = actor(obs, actions)\n",
        "        log_ratio = log_probs - old_log_probs\n",
        "        ratio = torch.exp(log_ratio)\n",
        "        \n",
        "        clipped = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
        "        loss = torch.mean(-torch.min(ratio * adv, clipped * adv)) - entropy_coeff * entropy.mean()\n",
        "\n",
        "        kl = (torch.exp(log_probs) * log_ratio).mean()\n",
        "\n",
        "        return loss, kl\n",
        "\n",
        "    def train_epoch():\n",
        "        obs_batch = []\n",
        "        action_batch = []\n",
        "        log_probs = []\n",
        "        entropies = []\n",
        "        rewards_batch = []\n",
        "        advantages_batch = []\n",
        "        returns_batch = []\n",
        "        \n",
        "        obs, done = env.reset(), False\n",
        "        \n",
        "        episode_rewards = []\n",
        "        \n",
        "        while True:\n",
        "            \n",
        "            obs_batch.append(obs)\n",
        "            with torch.no_grad():\n",
        "              act, log_prob, entrpy = actor.sample_action(obs.unsqueeze(0))\n",
        "\n",
        "            obs, reward, done, _ = env.step(act + 2)\n",
        "\n",
        "            action_batch.append(act)\n",
        "            log_probs.append(log_prob)\n",
        "            entropies.append(entrpy)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                #pdb.set_trace()\n",
        "                returns_batch.append(sum(episode_rewards))\n",
        "                advantages, rewards = rewards_and_advantages(episode_rewards, obs_batch, critic)\n",
        "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "                rewards_batch.append(rewards)\n",
        "                advantages_batch.append(advantages)\n",
        "\n",
        "                obs, done = env.reset(), False\n",
        "                episode_rewards = []\n",
        "                if len(obs_batch) > batch_size:\n",
        "                    break\n",
        "        #pdb.set_trace()\n",
        "        entropy = torch.stack(entropies)\n",
        "        obs_tensor = torch.stack(obs_batch).data\n",
        "        adv_tensor = torch.cat(advantages_batch).data\n",
        "        reward_tensor = torch.cat(rewards_batch).data\n",
        "        action_tensor = torch.as_tensor(action_batch).data\n",
        "       \n",
        "        log_probabilities = torch.stack(log_probs)\n",
        "        old_log_probabilities = log_probabilities.detach() + 1e-8\n",
        "\n",
        "\n",
        "        loss_clip, kl_old_new = torch.tensor(0.0), torch.tensor(0.0)\n",
        "        print(f'Training policy (actor) for {train_actor_iters} iterations')\n",
        "        for i in range(1, train_actor_iters + 1):\n",
        "            actor_opt.zero_grad()\n",
        "\n",
        "            loss_clip, kl = loss_and_kl(obs=obs_tensor, adv=adv_tensor, actions=action_tensor, old_log_probs=old_log_probabilities)\n",
        "            if kl > 1.5 * max_kl:\n",
        "                 print(f'Early stopping at iteration {i}, reached max KL')\n",
        "                 break\n",
        "            kl_old_new = kl\n",
        "            loss_clip.backward()\n",
        "            actor_opt.step()\n",
        "\n",
        "\n",
        "        # provjeri za nan i inf?\n",
        "        print(f'Training value function (critic) for {train_critic_iters} iterations')\n",
        "        for _ in range(train_critic_iters):\n",
        "            value_opt.zero_grad()\n",
        "            value_loss = torch.mean((critic(obs_tensor) - reward_tensor) ** 2) * val_loss_coef\n",
        "            value_loss.backward()\n",
        "            value_opt.step()\n",
        "\n",
        "        return loss_clip, value_loss, returns_batch, kl_old_new\n",
        "    \n",
        "    for i in range(1, epochs + 1):\n",
        "        policy_loss, value_loss, returns, kl = train_epoch()\n",
        "        print('Epoch: %3d \\t Policy loss: %.3f \\t Value loss: %.3f \\t Avg Return: %.3f \\t KL: %.3f'%\n",
        "                (i, policy_loss, value_loss, torch.mean(torch.as_tensor(returns)), kl))\n",
        "    return actor, critic\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYN4xaaRNNvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from core import CategoricalActor, make_ann\n",
        "\n",
        "def make_actor(n_actions, cnn_activation=nn.ReLU, ann_activation=nn.ReLU):\n",
        "  conv_layers = []\n",
        "  # conv1 : 80 x 80 -> 40 x 40\n",
        "  conv_layers.append(nn.Conv2d(4, 8, kernel_size=2, stride=2))\n",
        "  conv_layers.append(cnn_activation())\n",
        "  # conv1 : 40 x 40 -> 20 x 20\n",
        "  conv_layers.append(nn.Conv2d(8, 16, kernel_size=2, stride=2))\n",
        "  conv_layers.append(cnn_activation())\n",
        "  # conv1 : 20 x 20 -> 10 x 10\n",
        "  conv_layers.append(nn.Conv2d(16, 32, kernel_size=2, stride=2))\n",
        "  conv_layers.append(cnn_activation())\n",
        "  # conv1 : 10 x 10 -> 5 x 5\n",
        "  conv_layers.append(nn.Conv2d(32, 64, kernel_size=2, stride=2))\n",
        "  conv_layers.append(cnn_activation())\n",
        "  conv_layers.append(nn.Flatten())\n",
        "\n",
        "  return CategoricalActor(make_ann([64 * 5 * 5, 512, 256, 128, n_actions],\n",
        "                          ann_activation,\n",
        "                          conv_layers))\n",
        "\n",
        "def make_critic(cnn_activation=nn.ReLU, ann_activation=nn.Tanh):\n",
        "  conv_layers = []\n",
        "  # conv1 : 80 x 80 -> 40 x 40\n",
        "  conv_layers.append(nn.Conv2d(4, 8, kernel_size=2, stride=2))\n",
        "  conv_layers.append(cnn_activation())\n",
        "  # conv1 : 40 x 40 -> 20 x 20\n",
        "  conv_layers.append(nn.Conv2d(8, 16, kernel_size=2, stride=2))\n",
        "  conv_layers.append(cnn_activation())\n",
        "  # conv1 : 20 x 20 -> 10 x 10\n",
        "  conv_layers.append(nn.Conv2d(16, 32, kernel_size=2, stride=2))\n",
        "  conv_layers.append(cnn_activation())\n",
        "  # conv1 : 10 x 10 -> 5 x 5\n",
        "  conv_layers.append(nn.Conv2d(32, 64, kernel_size=2, stride=2))\n",
        "  conv_layers.append(cnn_activation())\n",
        "  conv_layers.append(nn.Flatten())\n",
        "\n",
        "  return make_ann([64 * 5 * 5, 512, 256, 128, 1], ann_activation, conv_layers)\n",
        "\n",
        "def main(pong_env_name, use_skip=True, epochs=1000, batch_size=50):\n",
        "  env = make_pong_env(pong_env_name, use_skip)\n",
        "  # Pong ima action space velicine 6, ali 2 redundantne i 2 beskorisne akcije\n",
        "  actor = make_actor(env.action_space.n - 4)\n",
        "  critic = make_critic()\n",
        "\n",
        "  opt_val = optim.SGD(actor.parameters(), lr=3e-4)\n",
        "  opt_policy = optim.SGD(critic.parameters(), lr=3e-4)\n",
        "\n",
        "  actor, critic = train(env, actor, critic, opt_val, opt_policy, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "  return actor, critic\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JEiQmCgq2r0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "def test(env_name, actor, directory, episodes):\n",
        "  actor.eval()\n",
        "  display = Display(visible=0, size=(400, 300))\n",
        "  display.start()\n",
        "  env = gym.make(env_name)\n",
        "  #env = gym.wrappers.Monitor(env, directory, video_callable=lambda id: True, force=True)\n",
        "  env = SkipAndPoolWrapper(env)\n",
        "  env = FireResetWrapper(env)\n",
        "  env = PreprocessWrapper(env)\n",
        "  env = BufferedWrapper(env, 4)\n",
        "  env = TorchWrapper(env)\n",
        "  \n",
        "\n",
        "  #env = gym.wrappers.Monitor(env, directory, force=True)\n",
        "  for _ in range(episodes):\n",
        "    obs, done = env.reset(), False\n",
        "    #env.render()\n",
        "    while not done:\n",
        "\n",
        "      with torch.no_grad():\n",
        "        obs, _, done, _ = env.step(actor.sample_action(obs.unsqueeze(0))[0] + 2)\n",
        "        screen = env.render(mode='rgb_array')\n",
        "\n",
        "        plt.imshow(screen)\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "        ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  env.close()\n",
        "  display.stop()\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b889BzZOSMfs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c61ed22d-320a-45c5-bfe3-30d934f3ef77"
      },
      "source": [
        "actor, critic = main('PongNoFrameskip-v4', epochs=4000, batch_size=1000)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:   1 \t Policy loss: -0.000 \t Value loss: 2.818 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:   2 \t Policy loss: -0.000 \t Value loss: 2.073 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:   3 \t Policy loss: -0.001 \t Value loss: 2.198 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:   4 \t Policy loss: -0.001 \t Value loss: 2.664 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:   5 \t Policy loss: -0.000 \t Value loss: 2.175 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:   6 \t Policy loss: -0.001 \t Value loss: 1.362 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:   7 \t Policy loss: -0.000 \t Value loss: 2.166 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:   8 \t Policy loss: -0.001 \t Value loss: 1.472 \t Avg Return: -18.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:   9 \t Policy loss: -0.001 \t Value loss: 2.876 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  10 \t Policy loss: -0.001 \t Value loss: 2.138 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  11 \t Policy loss: -0.001 \t Value loss: 2.615 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  12 \t Policy loss: -0.001 \t Value loss: 2.311 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  13 \t Policy loss: -0.001 \t Value loss: 2.056 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  14 \t Policy loss: -0.001 \t Value loss: 2.528 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  15 \t Policy loss: -0.001 \t Value loss: 1.711 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  16 \t Policy loss: -0.001 \t Value loss: 1.495 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  17 \t Policy loss: -0.001 \t Value loss: 1.653 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  18 \t Policy loss: -0.001 \t Value loss: 2.075 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  19 \t Policy loss: -0.000 \t Value loss: 1.895 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  20 \t Policy loss: -0.000 \t Value loss: 2.227 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  21 \t Policy loss: -0.000 \t Value loss: 1.752 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  22 \t Policy loss: -0.001 \t Value loss: 1.871 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  23 \t Policy loss: -0.001 \t Value loss: 2.588 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  24 \t Policy loss: -0.001 \t Value loss: 2.285 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  25 \t Policy loss: -0.001 \t Value loss: 2.132 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  26 \t Policy loss: -0.001 \t Value loss: 1.832 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  27 \t Policy loss: -0.001 \t Value loss: 1.292 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  28 \t Policy loss: -0.000 \t Value loss: 2.291 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  29 \t Policy loss: -0.001 \t Value loss: 2.352 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  30 \t Policy loss: -0.001 \t Value loss: 1.537 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  31 \t Policy loss: -0.001 \t Value loss: 2.047 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  32 \t Policy loss: -0.000 \t Value loss: 1.365 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  33 \t Policy loss: -0.000 \t Value loss: 2.170 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  34 \t Policy loss: -0.001 \t Value loss: 2.513 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  35 \t Policy loss: -0.000 \t Value loss: 2.038 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  36 \t Policy loss: -0.001 \t Value loss: 2.009 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  37 \t Policy loss: -0.001 \t Value loss: 3.055 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  38 \t Policy loss: -0.000 \t Value loss: 2.054 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  39 \t Policy loss: -0.000 \t Value loss: 1.918 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  40 \t Policy loss: -0.000 \t Value loss: 2.226 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  41 \t Policy loss: -0.001 \t Value loss: 1.904 \t Avg Return: -19.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  42 \t Policy loss: -0.000 \t Value loss: 2.553 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  43 \t Policy loss: -0.001 \t Value loss: 2.500 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  44 \t Policy loss: -0.000 \t Value loss: 2.118 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  45 \t Policy loss: -0.000 \t Value loss: 1.837 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  46 \t Policy loss: -0.001 \t Value loss: 1.891 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  47 \t Policy loss: -0.000 \t Value loss: 1.838 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  48 \t Policy loss: -0.000 \t Value loss: 2.670 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  49 \t Policy loss: -0.001 \t Value loss: 1.312 \t Avg Return: -18.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  50 \t Policy loss: -0.001 \t Value loss: 2.612 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  51 \t Policy loss: -0.001 \t Value loss: 1.961 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  52 \t Policy loss: -0.000 \t Value loss: 1.944 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  53 \t Policy loss: -0.000 \t Value loss: 1.330 \t Avg Return: -17.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  54 \t Policy loss: -0.001 \t Value loss: 1.587 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  55 \t Policy loss: -0.001 \t Value loss: 2.539 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  56 \t Policy loss: -0.001 \t Value loss: 2.173 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  57 \t Policy loss: -0.001 \t Value loss: 2.183 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  58 \t Policy loss: -0.000 \t Value loss: 3.037 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  59 \t Policy loss: -0.001 \t Value loss: 2.171 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  60 \t Policy loss: -0.001 \t Value loss: 1.733 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  61 \t Policy loss: -0.001 \t Value loss: 2.400 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  62 \t Policy loss: -0.000 \t Value loss: 1.140 \t Avg Return: -17.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  63 \t Policy loss: -0.001 \t Value loss: 1.711 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  64 \t Policy loss: -0.001 \t Value loss: 1.360 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  65 \t Policy loss: -0.001 \t Value loss: 2.051 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  66 \t Policy loss: -0.001 \t Value loss: 2.554 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  67 \t Policy loss: -0.001 \t Value loss: 2.765 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  68 \t Policy loss: -0.001 \t Value loss: 1.572 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  69 \t Policy loss: -0.000 \t Value loss: 1.641 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  70 \t Policy loss: -0.001 \t Value loss: 2.126 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  71 \t Policy loss: -0.001 \t Value loss: 2.154 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  72 \t Policy loss: -0.001 \t Value loss: 2.234 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  73 \t Policy loss: -0.001 \t Value loss: 2.029 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  74 \t Policy loss: -0.001 \t Value loss: 1.873 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  75 \t Policy loss: -0.001 \t Value loss: 2.415 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  76 \t Policy loss: -0.000 \t Value loss: 2.181 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  77 \t Policy loss: -0.001 \t Value loss: 3.053 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  78 \t Policy loss: -0.001 \t Value loss: 2.612 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  79 \t Policy loss: -0.001 \t Value loss: 1.908 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  80 \t Policy loss: -0.001 \t Value loss: 2.120 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  81 \t Policy loss: -0.001 \t Value loss: 1.859 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  82 \t Policy loss: -0.000 \t Value loss: 1.943 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  83 \t Policy loss: -0.000 \t Value loss: 2.584 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  84 \t Policy loss: -0.001 \t Value loss: 1.995 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  85 \t Policy loss: -0.001 \t Value loss: 2.725 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  86 \t Policy loss: -0.001 \t Value loss: 2.519 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  87 \t Policy loss: -0.000 \t Value loss: 2.297 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  88 \t Policy loss: -0.001 \t Value loss: 2.192 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  89 \t Policy loss: -0.001 \t Value loss: 2.282 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  90 \t Policy loss: -0.001 \t Value loss: 2.515 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  91 \t Policy loss: -0.001 \t Value loss: 1.825 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  92 \t Policy loss: -0.001 \t Value loss: 3.157 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  93 \t Policy loss: -0.001 \t Value loss: 2.225 \t Avg Return: -20.500 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  94 \t Policy loss: -0.000 \t Value loss: 2.106 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  95 \t Policy loss: -0.000 \t Value loss: 2.237 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  96 \t Policy loss: -0.000 \t Value loss: 1.163 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  97 \t Policy loss: -0.001 \t Value loss: 1.557 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  98 \t Policy loss: -0.001 \t Value loss: 1.894 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch:  99 \t Policy loss: -0.000 \t Value loss: 2.136 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch: 100 \t Policy loss: -0.000 \t Value loss: 1.519 \t Avg Return: -19.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch: 101 \t Policy loss: -0.000 \t Value loss: 2.234 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch: 102 \t Policy loss: -0.001 \t Value loss: 3.160 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch: 103 \t Policy loss: -0.000 \t Value loss: 2.075 \t Avg Return: -20.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n",
            "Training value function (critic) for 5 iterations\n",
            "Epoch: 104 \t Policy loss: -0.001 \t Value loss: 1.854 \t Avg Return: -21.000 \t KL: 0.000\n",
            "Training policy (actor) for 5 iterations\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tSe4iOjrjZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test('PongNoFrameskip-v4', actor, 'prob_dir', 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}