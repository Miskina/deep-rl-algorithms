{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ppo_pong2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKL5ccGidMnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# http://karpathy.github.io/2016/05/31/rl/\n",
        "def preprocess_image(img):\n",
        "  # shape = 210x160x3\n",
        "  img = img[35:195] # crop -> 160 x 160 x 3\n",
        "  img = img[::2,::2,0] # downsample by factor of 2 -> 80 x 80\n",
        "  img[img == 144] = 0 # erase background (background type 1)\n",
        "  img[img == 109] = 0 # erase background (background type 2)\n",
        "  img[img != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  return torch.from_numpy(img.ravel()).to(dtype=torch.float).unsqueeze(0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAIoxVfXqqAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(env,\n",
        "          actor,\n",
        "          critic,\n",
        "          value_opt,\n",
        "          actor_opt,\n",
        "          max_kl=0.01,\n",
        "          epsilon=0.2,\n",
        "          epochs=100,\n",
        "          batch_size=50,\n",
        "          gamma=0.99,\n",
        "          lam=0.96,\n",
        "          val_loss_coef=0.5,\n",
        "          entropy_coeff=0.001,\n",
        "          train_critic_iters=5,\n",
        "          train_actor_iters=5):\n",
        "\n",
        "    \n",
        "    def rewards_and_advantages(rewards, obs, critic):\n",
        "        #pdb.set_trace()\n",
        "        n = len(rewards)\n",
        "        adv_to_go = torch.zeros(n)\n",
        "        rewards_to_go = torch.zeros(n)\n",
        "        #vs = torch.zeros(n)\n",
        "        #for i in range(n):\n",
        "        #   vs[i] = critic(obs[i])\n",
        "        with torch.no_grad():\n",
        "          vs = critic(torch.stack(obs)).squeeze()\n",
        "\n",
        "        for i in reversed(range(n)):\n",
        "            not_last = i < (n - 1)\n",
        "            rewards_to_go[i] = rewards[i]\n",
        "            adv_to_go[i] = rewards[i] - vs[i]\n",
        "\n",
        "            if not_last:\n",
        "                #pdb.set_trace()\n",
        "                adv_to_go[i] += gamma * vs[i + 1] + gamma * lam * adv_to_go[i + 1]\n",
        "                rewards_to_go[i] += gamma * rewards_to_go[i + 1]\n",
        "            \n",
        "        return adv_to_go, rewards_to_go\n",
        "    \n",
        "\n",
        "    def loss_and_kl(obs, adv, actions, old_log_probs):\n",
        "        log_probs, entropy = actor(obs, actions)\n",
        "        log_ratio = log_probs - old_log_probs\n",
        "        ratio = torch.exp(log_ratio)\n",
        "        \n",
        "        clipped = torch.clamp(ratio, 1-epsilon, 1+epsilon)\n",
        "        loss = torch.mean(-torch.min(ratio * adv, clipped * adv)) + entropy_coeff * entropy.mean()\n",
        "\n",
        "        kl = (torch.exp(log_probs) * log_ratio).mean()\n",
        "\n",
        "        return loss, kl\n",
        "\n",
        "    def train_epoch():\n",
        "\n",
        "        obs_batch = []\n",
        "        action_batch = []\n",
        "        log_probs = []\n",
        "        entropies = []\n",
        "        rewards_batch = []\n",
        "        advantages_batch = []\n",
        "        returns_batch = []\n",
        "        \n",
        "        curr_obs, done = env.reset(), False\n",
        "        prev_obs = 0.0\n",
        "        \n",
        "        episode_rewards = []\n",
        "        \n",
        "        while True:\n",
        "            \n",
        "            obs = curr_obs - prev_obs\n",
        "            obs_batch.append(obs)\n",
        "            prev_obs = curr_obs\n",
        "\n",
        "            with torch.no_grad():\n",
        "              act, log_prob, entrpy = actor.sample_action(obs)\n",
        "\n",
        "            \n",
        "            curr_obs, reward, done, _ = env.step(act + 2)\n",
        "\n",
        "            action_batch.append(act)\n",
        "            log_probs.append(log_prob)\n",
        "            entropies.append(entrpy)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "            if done:\n",
        "                #pdb.set_trace()\n",
        "                returns_batch.append(sum(episode_rewards))\n",
        "                advantages, rewards = rewards_and_advantages(episode_rewards, obs_batch, critic)\n",
        "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "                rewards_batch.append(rewards)\n",
        "                advantages_batch.append(advantages)\n",
        "\n",
        "                prev_obs = 0.0\n",
        "                curr_obs, done = env.reset(), False\n",
        "                episode_rewards = []\n",
        "                if len(obs_batch) > batch_size:\n",
        "                    break\n",
        "        #pdb.set_trace()\n",
        "        entropy = torch.stack(entropies)\n",
        "        obs_tensor = torch.stack(obs_batch).data\n",
        "        adv_tensor = torch.cat(advantages_batch).data\n",
        "        reward_tensor = torch.cat(rewards_batch).data\n",
        "        action_tensor = torch.as_tensor(action_batch).data\n",
        "       \n",
        "        log_probabilities = torch.stack(log_probs)\n",
        "        old_log_probabilities = log_probabilities.detach() + 1e-8\n",
        "\n",
        "\n",
        "        loss_clip, kl_old_new = torch.tensor(0.0), torch.tensor(0.0)\n",
        "        print(f'Training policy (actor) for {train_actor_iters} iterations')\n",
        "        for i in range(1, train_actor_iters + 1):\n",
        "            actor_opt.zero_grad()\n",
        "\n",
        "            loss_clip, kl = loss_and_kl(obs=obs_tensor, adv=adv_tensor, actions=action_tensor, old_log_probs=old_log_probabilities)\n",
        "            if kl > 1.5 * max_kl:\n",
        "                 print(f'Early stopping at iteration {i}, reached max KL')\n",
        "                 break\n",
        "            kl_old_new = kl\n",
        "            loss_clip.backward()\n",
        "            actor_opt.step()\n",
        "\n",
        "\n",
        "        # provjeri za nan i inf?\n",
        "        print(f'Training value function (critic) for {train_critic_iters} iterations')\n",
        "        for _ in range(train_critic_iters):\n",
        "            value_opt.zero_grad()\n",
        "            value_loss = torch.mean((critic(obs_tensor) - reward_tensor) ** 2) * val_loss_coef\n",
        "            value_loss.backward()\n",
        "            value_opt.step()\n",
        "\n",
        "        return loss_clip, value_loss, returns_batch, kl_old_new\n",
        "    \n",
        "    for i in range(1, epochs + 1):\n",
        "        policy_loss, value_loss, returns, kl = train_epoch()\n",
        "        print('Epoch: %3d \\t Policy loss: %.3f \\t Value loss: %.3f \\t Avg Return: %.3f \\t KL: %.3f'%\n",
        "                (i, policy_loss, value_loss, torch.mean(torch.as_tensor(returns)), kl))\n",
        "    return actor, critic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKiaxGIC9-Nu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "\n",
        "class TorchWrapper(gym.Wrapper):\n",
        "    \n",
        "    def __init__(self, env=None, obs_dtype=torch.float32, reward_dtype=torch.float32):\n",
        "        super(TorchWrapper, self).__init__(env)\n",
        "        self.obs_dtype = obs_dtype\n",
        "        self.reward_dtype = reward_dtype\n",
        "    \n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        return torch.as_tensor(obs, dtype=self.obs_dtype)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        return torch.as_tensor(obs, dtype=self.obs_dtype), torch.as_tensor(reward, dtype=self.reward_dtype), done, info\n",
        "\n",
        "class PreprocessWrapper(gym.ObservationWrapper):\n",
        "\n",
        "  def __init__(self, env=None):\n",
        "    super(PreprocessWrapper, self).__init__(env)\n",
        "    obs_space_old = self.observation_space\n",
        "    old_shape = obs_space_old.shape\n",
        "    self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(int((old_shape[0] - 50) / 2), int((old_shape[1]) / 2)))\n",
        "    \n",
        "  \n",
        "  def observation(self, obs):\n",
        "    return preprocess_image(obs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMQavO9h8wRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "f330b4fb-de2d-4a0b-9354-0d7986889ac4"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from core import make_ann, CategoricalActor\n",
        "\n",
        "\n",
        "def main(epochs=10, batch_size=100):\n",
        "  env = gym.make('Pong-v0')\n",
        "  env = PreprocessWrapper(env)\n",
        "  env = TorchWrapper(env)\n",
        "  actor = CategoricalActor(make_ann([80 * 80, 512, 256, env.action_space.n - 4], nn.ReLU))\n",
        "  critic = make_ann([80*80, 512, 256, 128, 1], nn.ReLU)\n",
        "\n",
        "  opt_val = optim.SGD(actor.parameters(), lr=3e-4)\n",
        "  opt_policy = optim.SGD(critic.parameters(), lr=3e-4)\n",
        "\n",
        "  actor, critic = train(env, actor, critic, opt_val, opt_policy, epochs=epochs, batch_size=batch_size)\n",
        "\n",
        "  return actor, critic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-110100563528>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_ann\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalActor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'core'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccf1N4M5BbVJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "\n",
        "def test(env_name, actor, directory, episodes):\n",
        "  actor.eval()\n",
        "  display = Display(visible=0, size=(400, 300))\n",
        "  display.start()\n",
        "  env = gym.make(env_name)\n",
        "  #env = gym.wrappers.Monitor(env, directory, video_callable=lambda id: True, force=True)\n",
        "  env = SkipAndPoolWrapper(env)\n",
        "  env = FireResetWrapper(env)\n",
        "  env = PreprocessWrapper(env)\n",
        "  env = BufferedWrapper(env, 4)\n",
        "  env = TorchWrapper(env)\n",
        "  \n",
        "\n",
        "  #env = gym.wrappers.Monitor(env, directory, force=True)\n",
        "  for _ in range(episodes):\n",
        "    obs, done = env.reset(), False\n",
        "    #env.render()\n",
        "    while not done:\n",
        "\n",
        "      with torch.no_grad():\n",
        "        obs, _, done, _ = env.step(actor.sample_action(obs.unsqueeze(0))[0] + 2)\n",
        "        screen = env.render(mode='rgb_array')\n",
        "\n",
        "        plt.imshow(screen)\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "        ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  env.close()\n",
        "  display.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNdTm0B19xNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actor, critic = main(5000, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHvY00mcBk3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test('Pong-v0', actor, 'dir', 1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}